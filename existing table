from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("CreatePartitionedTableAndInsertData") \
    .enableHiveSupport() \
    .getOrCreate()

# Define the source Hive table and destination partitioned table names
source_hive_table_name = "hlx_wkly"
destination_partitioned_table_name = "darshan.partitioned_table"

# Read the schema of the source Hive table
source_schema = spark.sql(f"DESCRIBE {source_hive_table_name}")
column_names = [row.col_name for row in source_schema.collect()]

# Create the destination partitioned table with the same structure as the source table
create_table_query = f"""
    CREATE TABLE IF NOT EXISTS {destination_partitioned_table_name} (
        {", ".join(column_names)}
    )
    PARTITIONED BY (pp_month STRING)
    STORED AS PARQUET
"""
spark.sql(create_table_query)

# Insert data into the partitioned table
insert_query = f"""
    INSERT INTO {destination_partitioned_table_name} PARTITION(pp_month)
    SELECT *, pp_month FROM {source_hive_table_name}
"""
spark.sql(insert_query)

# Stop the Spark session
spark.stop()
