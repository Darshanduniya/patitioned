from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create Spark session
spark = SparkSession.builder \
    .appName("HiveTableToNewTable") \
    .enableHiveSupport() \
    .getOrCreate()

# Read data from Hive table
hive_table_name = "your_hive_table_name"
df = spark.table(hive_table_name)

# Create new table with partition column
new_table_name = "new_table"
partition_column = "Pp_month"
schema = df.schema
spark.sql(f"CREATE TABLE IF NOT EXISTS {new_table_name} ({', '.join([f'{col.name} {col.dataType}' for col in schema])}) PARTITIONED BY ({partition_column} STRING)")

# Upsert data into new table
def upsert_data(target_df, source_df, partition_col):
    unique_key_col = "unique_id"
    source_df = source_df.drop(partition_col)
    target_df.alias("target") \
        .merge(source_df.alias("source"), f"target.{unique_key_col} = source.{unique_key_col}") \
        .whenMatchedUpdateAll() \
        .whenNotMatchedInsertAll() \
        .execute()

upsert_data(
    target_df=spark.table(new_table_name),
    source_df=df,
    partition_col=partition_column
)
